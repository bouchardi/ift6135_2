{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q5_2_plot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RSH0M6RwXkme",
        "colab_type": "code",
        "outputId": "19b8d3d9-8610-4de9-c5e6-f02c9e353c74",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7a50be33-b9ee-452f-b90a-e75b11d7c32d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7a50be33-b9ee-452f-b90a-e75b11d7c32d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0.zip to GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0.zip\n",
            "User uploaded file \"GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0.zip\" with length 141862277 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_c4dqbQQdz50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "45517a81-6be6-463f-982f-9ae1056985c2"
      },
      "cell_type": "code",
      "source": [
        "!unzip GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0.zip"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0.zip\n",
            "   creating: GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/\n",
            "  inflating: GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/exp_config.txt  \n",
            "   creating: __MACOSX/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/\n",
            "  inflating: __MACOSX/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/._exp_config.txt  \n",
            "  inflating: GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/log.txt  \n",
            "  inflating: __MACOSX/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/._log.txt  \n",
            "  inflating: GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/best_params.pt  \n",
            "  inflating: __MACOSX/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/._best_params.pt  \n",
            "  inflating: GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/learning_curves.npy  \n",
            "  inflating: __MACOSX/GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0/._learning_curves.npy  \n",
            "  inflating: __MACOSX/._GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "43A6qx_vZBQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "  \"A helper function for producing N identical layers (each with their own parameters).\"\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "# NOTE ==============================================\n",
        "#\n",
        "# Fill in code for every method which has a TODO\n",
        "#\n",
        "# Your implementation should use the contract (inputs\n",
        "# and outputs) given for each model, because that is\n",
        "# what the main script expects. If you modify the contract,\n",
        "# you must justify that choice, note it in your report, and notify the TAs\n",
        "# so that we run the correct code.\n",
        "#\n",
        "# You may modify the internals of the RNN and GRU classes\n",
        "# as much as you like, except you must keep the methods\n",
        "# in each (init_weights_uniform, init_hidden, and forward)\n",
        "# Using nn.Module and \"forward\" tells torch which\n",
        "# parameters are involved in the forward pass, so that it\n",
        "# can correctly (automatically) set up the backward pass.\n",
        "#\n",
        "# You should not modify the interals of the Transformer\n",
        "# except where indicated to implement the multi-head\n",
        "# attention.\n",
        "\n",
        "\n",
        "class GRU_cell(nn.Module): # Implement a stacked GRU RNN\n",
        "  \"\"\"\n",
        "  Follow the same instructions as for RNN (above), but use the equations for\n",
        "  GRU, not Vanilla RNN.\n",
        "  \"\"\"\n",
        "  def __init__(self, emb_size, hidden_size):\n",
        "    super(GRU_cell, self).__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.W_x = nn.Parameter(torch.Tensor(emb_size,3*hidden_size))\n",
        "    self.U_h = nn.Parameter(torch.Tensor(hidden_size,2*hidden_size))\n",
        "    self.U_h_tilde = nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
        "    self.bias_rzh = nn.Parameter(torch.Tensor(3*hidden_size))\n",
        "\n",
        "    self.init_weights_uniform()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "  def init_weights_uniform(self):\n",
        "    # Initialize all other (i.e. recurrent and linear) weights AND biases uniformly\n",
        "    # in the range [-k, k] where k is the square root of 1/hidden_size\n",
        "    k = math.sqrt(1.0/self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-k, k)\n",
        "\n",
        "    torch.nn.init.zeros_(self.bias_rzh)\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        - inputs: A mini-batch of input sequences, composed of integers that\n",
        "                    represent the index of the current token(s) in the vocabulary.\n",
        "                        shape: (batch_size, embedding)\n",
        "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
        "                        shape: (batch_size, hidden_size)\n",
        "    \"\"\"\n",
        "    batch_size = hidden.size(0)\n",
        "\n",
        "    bias_rzh_batch = self.bias_rzh.unsqueeze(0).expand(batch_size, self.bias_rzh.size(0))\n",
        "    W_x = torch.addmm(bias_rzh_batch,inputs,self.W_x)\n",
        "    U_h_prev = torch.mm(hidden,self.U_h)\n",
        "    W_rx, W_zx, W_hx = torch.split(W_x,self.hidden_size, dim=1)\n",
        "    U_rh, U_zh = torch.split(U_h_prev,self.hidden_size, dim=1)\n",
        "\n",
        "    r = self.sigmoid(W_rx + U_rh)\n",
        "    z = self.sigmoid(W_zx + U_zh)\n",
        "    h_tilde = self.tanh(W_hx + torch.mm(r * hidden,self.U_h_tilde))\n",
        "    h = ((1-z) * hidden) + (z * h_tilde)\n",
        "\n",
        "    return h\n",
        "\n",
        "# Problem 2\n",
        "class GRU(nn.Module): # Implement a stacked GRU RNN\n",
        "  \"\"\"\n",
        "  Follow the same instructions as for RNN (above), but use the equations for\n",
        "  GRU, not Vanilla RNN.\n",
        "  \"\"\"\n",
        "  def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
        "    \"\"\"\n",
        "    emb_size:     The number of units in the input embeddings\n",
        "    hidden_size:  The number of hidden units per layer\n",
        "    seq_len:      The length of the input sequences\n",
        "    vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)\n",
        "    num_layers:   The depth of the stack (i.e. the number of hidden layers at\n",
        "                  each time-step)\n",
        "    dp_keep_prob: The probability of *not* dropping out units in the\n",
        "                  non-recurrent connections.\n",
        "                  Do not apply dropout on recurrent connections.\n",
        "    \"\"\"\n",
        "    super(GRU, self).__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.seq_len = seq_len\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Keep hidden layers result when we want to compute the avg gradients\n",
        "    self.hiddens = []\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "    self.decode = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(1 - dp_keep_prob)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "    self.gen_softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    # Weight initialization (Embedding has no bias)\n",
        "    self.init_weights_uniform(self.embedding, init_bias=False)\n",
        "    self.init_weights_uniform(self.decode, init_bias=True)\n",
        "\n",
        "    self.GRU_cells = nn.ModuleList([GRU_cell(emb_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])\n",
        "\n",
        "    # Initialization of the parameters of the recurrent and fc layers.\n",
        "    # Your implementation should support any number of stacked hidden layers\n",
        "    # (specified by num_layers), use an input embedding layer, and include fully\n",
        "    # connected layers with dropout after each recurrent layer.\n",
        "    # Note: you may use pytorch's nn.Linear, nn.Dropout, and nn.Embedding\n",
        "    # modules, but not recurrent modules.\n",
        "    #\n",
        "    # To create a variable number of parameter tensors and/or nn.Modules\n",
        "    # (for the stacked hidden layer), you may need to use nn.ModuleList or the\n",
        "    # provided clones function (as opposed to a regular python list), in order\n",
        "    # for Pytorch to recognize these parameters as belonging to this nn.Module\n",
        "    # and compute their gradients automatically. You're not obligated to use the\n",
        "    # provided clones function.\n",
        "\n",
        "  def init_weights_uniform(self, layer, init_bias=False):\n",
        "    \"\"\"\n",
        "    # Initialize the embedding and output weights uniformly in the range [-0.1, 0.1]\n",
        "    # and output biases to 0 (in place). The embeddings should not use a bias vector.\n",
        "    \"\"\"\n",
        "    torch.nn.init.uniform_(layer.weight, a=-0.1, b=0.1)\n",
        "    if init_bias:\n",
        "        torch.nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "  def init_hidden(self):\n",
        "    # initialize the hidden states to zero\n",
        "    \"\"\"\n",
        "    This is used for the first mini-batch in an epoch, only.\n",
        "    \"\"\"\n",
        "    return torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "  def forward(self, inputs, hidden, keep_hiddens=False):\n",
        "    # Compute the forward pass, using a nested python for loops.\n",
        "    # The outer for loop should iterate over timesteps, and the\n",
        "    # inner for loop should iterate over hidden layers of the stack.\n",
        "    #\n",
        "    # Within these for loops, use the parameter tensors and/or nn.modules you\n",
        "    # created in __init__ to compute the recurrent updates according to the\n",
        "    # equations provided in the .tex of the assignment.\n",
        "    #\n",
        "    # Note that those equations are for a single hidden-layer RNN, not a stacked\n",
        "    # RNN. For a stacked RNN, the hidden states of the l-th layer are used as\n",
        "    # inputs to to the {l+1}-st layer (taking the place of the input sequence).\n",
        "\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        - inputs: A mini-batch of input sequences, composed of integers that\n",
        "                    represent the index of the current token(s) in the vocabulary.\n",
        "                        shape: (seq_len, batch_size)\n",
        "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
        "                        shape: (num_layers, batch_size, hidden_size)\n",
        "    Returns:\n",
        "        - Logits for the softmax over output tokens at every time-step.\n",
        "              **Do NOT apply softmax to the outputs!**\n",
        "              Pytorch's CrossEntropyLoss function (applied in ptb-lm.py) does\n",
        "              this computation implicitly.\n",
        "                    shape: (seq_len, batch_size, vocab_size)\n",
        "        - The final hidden states for every layer of the stacked RNN.\n",
        "              These will be used as the initial hidden states for all the\n",
        "              mini-batches in an epoch, except for the first, where the return\n",
        "              value of self.init_hidden will be used.\n",
        "              See the repackage_hiddens function in ptb-lm.py for more details,\n",
        "              if you are curious.\n",
        "                    shape: (num_layers, batch_size, hidden_size)\n",
        "    \"\"\"\n",
        "    self.hiddens = []\n",
        "    h_previous_ts = hidden\n",
        "    logits = []\n",
        "    embeddings = self.embedding(inputs)\n",
        "    for t in range(self.seq_len):\n",
        "      h_next_ts = []\n",
        "      input = self.dropout(embeddings[t])\n",
        "      for h_index in range(self.num_layers):\n",
        "        # Recurrent GRU cell\n",
        "        h_recurrent = self.GRU_cells[h_index].forward(input, h_previous_ts[h_index])\n",
        "        if keep_hiddens:\n",
        "            self.hiddens.append(h_recurrent)\n",
        "        # Fully connected layer with dropout\n",
        "        h_previous_layer = self.dropout(h_recurrent)\n",
        "        input = h_previous_layer # used vertically up the layers\n",
        "        # Keep the ref for next ts\n",
        "        h_next_ts.append(h_recurrent) # used horizontally across timesteps\n",
        "      h_previous_ts = torch.stack(h_next_ts)\n",
        "      logits.append(self.decode(h_previous_layer))\n",
        "    return torch.stack(logits), h_next_ts\n",
        "\n",
        "  def generate(self, input, hidden, generated_seq_len, device):  # generate next word using the GRU\n",
        "    # Compute the forward pass, as in the self.forward method (above).\n",
        "    # You'll probably want to copy substantial portions of that code here.\n",
        "    #\n",
        "    # We \"seed\" the generation by providing the first inputs.\n",
        "    # Subsequent inputs are generated by sampling from the output distribution,\n",
        "    # as described in the tex (Problem 5.3)\n",
        "    # Unlike for self.forward, you WILL need to apply the softmax activation\n",
        "    # function here in order to compute the parameters of the categorical\n",
        "    # distributions to be sampled from at each time-step.\n",
        "\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        - input: A mini-batch of input tokens (NOT sequences!)\n",
        "                        shape: (batch_size)\n",
        "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
        "                        shape: (num_layers, batch_size, hidden_size)\n",
        "        - generated_seq_len: The length of the sequence to generate.\n",
        "                       Note that this can be different than the length used\n",
        "                       for training (self.seq_len)\n",
        "    Returns:\n",
        "        - Sampled sequences of tokens\n",
        "                    shape: (generated_seq_len, batch_size)\n",
        "    \"\"\"\n",
        "    self.eval()\n",
        "    input = input[0]\n",
        "    samples = []\n",
        "    h_previous_ts = hidden\n",
        "    new_input = input\n",
        "\n",
        "    for t in range(generated_seq_len):\n",
        "      h_next_ts = []\n",
        "      new_input = new_input.to(device)\n",
        "      embedding = self.embedding(new_input)\n",
        "      input = embedding\n",
        "      for h_index in range(self.num_layers):\n",
        "        # Recurrent GRU cell\n",
        "        h_recurrent = self.GRU_cells[h_index].forward(input, h_previous_ts[h_index])\n",
        "        # Fully connected layer with dropout\n",
        "        h_previous_layer = self.dropout(h_recurrent)\n",
        "        input = h_previous_layer  # used vertically up the layers\n",
        "        # Keep the ref for next ts\n",
        "        h_next_ts.append(h_recurrent)  # used horizontally across timesteps\n",
        "\n",
        "      h_previous_ts = torch.stack(h_next_ts)\n",
        "\n",
        "      sample = h_previous_layer\n",
        "      sample = self.gen_softmax(self.decode(sample))\n",
        "      sample_index = int(np.argmax(sample.cpu().detach().numpy()))\n",
        "      samples.append(sample_index)\n",
        "      new_input[0] = sample_index\n",
        "\n",
        "    return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7tD784mYbKp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# NOTE ==============================================\n",
        "#\n",
        "# Your implementation should use the contract (inputs\n",
        "# and outputs) given for each model, because that is\n",
        "# what the main script expects. If you modify the contract,\n",
        "# you must justify that choice, note it in your report, and notify the TAs\n",
        "# so that we run the correct code.\n",
        "#\n",
        "# You may modify the internals of the RNN and GRU classes\n",
        "# as much as you like, except you must keep the methods\n",
        "# in each (init_weights_uniform, init_hidden, and forward)\n",
        "# Using nn.Module and \"forward\" tells torch which\n",
        "# parameters are involved in the forward pass, so that it\n",
        "# can correctly (automatically) set up the backward pass.\n",
        "#\n",
        "# You should not modify the interals of the Transformer\n",
        "# except where indicated to implement the multi-head\n",
        "# attention.\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"A helper function for producing N identical layers (each with their own parameters).\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "# Problem 1\n",
        "class RNN(nn.Module): # Implement a stacked vanilla RNN with Tanh nonlinearities.\n",
        "  def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers=2, dp_keep_prob=0.5):\n",
        "\n",
        "    \"\"\"\n",
        "    emb_size:     The numvwe of units in the input embeddings\n",
        "    hidden_size:  The number of hidden units per layer\n",
        "    seq_len:      The length of the input sequences\n",
        "    vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)\n",
        "    num_layers:   The depth of the stack (i.e. the number of hidden layers at\n",
        "                  each time-step)\n",
        "    dp_keep_prob: The probability of *not* dropping out units in the\n",
        "                  non-recurrent connections.\n",
        "                  Do not apply dropout on recurrent connections.\n",
        "    \"\"\"\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.seq_len = seq_len\n",
        "    self.num_layers = num_layers\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # To compute the average gradient\n",
        "    self.hiddens = []\n",
        "\n",
        "    # Embedding encoder\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "    # N stacked recurrent layers (first layer has different input size)\n",
        "    linear_W0 = nn.Linear(emb_size, hidden_size)\n",
        "    self.init_weights_uniform_0(nn.Linear(hidden_size, hidden_size))\n",
        "    linear_W = [nn.Linear(hidden_size, hidden_size) for _ in range(num_layers - 1)]\n",
        "    for layer in linear_W:\n",
        "        self.init_weights_uniform_0(layer)\n",
        "    self.linear_W = nn.ModuleList([linear_W0] + linear_W)\n",
        "    linear_U = nn.Linear(hidden_size, hidden_size)\n",
        "    self.init_weights_uniform_0(linear_U)\n",
        "    self.linear_U = clones(linear_U, num_layers)\n",
        "\n",
        "    # Embedding decoder\n",
        "    self.decode = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(1 - dp_keep_prob)\n",
        "    self.activation = nn.Tanh()\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    # Weight initialization (Embedding has no bias)\n",
        "    self.init_weights_uniform(self.embedding, init_bias=False)\n",
        "    self.init_weights_uniform(self.decode, init_bias=True)\n",
        "\n",
        "  def init_weights_uniform(self, layer, init_bias=False):\n",
        "    \"\"\"\n",
        "    Initialize all the weights uniformly in the range [-0.1, 0.1]\n",
        "    and all the biases to 0 (in place)\n",
        "    \"\"\"\n",
        "    torch.nn.init.uniform_(layer.weight, a=-0.1, b=0.1)\n",
        "    if init_bias:\n",
        "        torch.nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "  def init_weights_uniform_0(self, layer, init_bias=True):\n",
        "    # Initialize all other (i.e. recurrent and linear) weights AND biases uniformly\n",
        "    # in the range [-k, k] where k is the square root of 1/hidden_size\n",
        "    k = math.sqrt(1.0/self.hidden_size)\n",
        "    torch.nn.init.uniform_(layer.weight, a=-k, b=k)\n",
        "    if init_bias:\n",
        "        torch.nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "\n",
        "  def init_hidden(self):\n",
        "    \"\"\"\n",
        "    This is used for the first mini-batch in an epoch, only.\n",
        "    \"\"\"\n",
        "    return torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "  def forward(self, inputs, hidden, keep_hiddens=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        - inputs: A mini-batch of input sequences, composed of integers that\n",
        "                    represent the index of the current token(s) in the vocabulary.\n",
        "                        shape: (seq_len, batch_size)\n",
        "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
        "                        shape: (num_layers, batch_size, hidden_size)\n",
        "    Returns:\n",
        "        - Logits for the softmax over output tokens at every time-step.\n",
        "              **Do NOT apply softmax to the outputs!**\n",
        "              Pytorch's CrossEntropyLoss function (applied in ptb-lm.py) does\n",
        "              this computation implicitly.\n",
        "                    shape: (seq_len, batch_size, vocab_size)\n",
        "        - The final hidden states for every layer of the stacked RNN.\n",
        "              These will be used as the initial hidden states for all the\n",
        "              mini-batches in an epoch, except for the first, where the return\n",
        "              value of self.init_hidden will be used.\n",
        "              See the repackage_hiddens function in ptb-lm.py for more details,\n",
        "              if you are curious.\n",
        "                    shape: (num_layers, batch_size, hidden_size)\n",
        "    \"\"\"\n",
        "    if keep_hiddens:\n",
        "        self.hiddens = []\n",
        "    h_previous_ts = hidden\n",
        "    seq_logits = []\n",
        "    emb = self.embedding(inputs)\n",
        "    for i in range(self.seq_len):\n",
        "        logits, h_previous_ts = self._forward_single_token_embedding(emb[i], h_previous_ts, keep_hiddens)\n",
        "        seq_logits.append(logits)\n",
        "    return torch.stack(seq_logits), h_previous_ts\n",
        "\n",
        "  def _forward_single_token_embedding(self, embedding, h_previous_ts, keep_hiddens=False):\n",
        "    \"\"\"\n",
        "    Forward pass for a single token embedding given the\n",
        "    hidden state at the previous time step\n",
        "    \"\"\"\n",
        "    h_next_ts = []\n",
        "    h_previous_layer = self.dropout(embedding)\n",
        "    for l in range(self.num_layers):\n",
        "        # Recurrent layer\n",
        "        a_W = self.linear_W[l](h_previous_layer)\n",
        "        a_U = self.linear_U[l](h_previous_ts[l])\n",
        "        h_recurrent = self.activation(a_U + a_W)\n",
        "        if keep_hiddens:\n",
        "            self.hiddens.append(h_recurrent)\n",
        "        # Fully connected layer\n",
        "        h_previous_layer = self.dropout(h_recurrent)\n",
        "        # Keep the ref for next ts\n",
        "        h_next_ts.append(h_recurrent)\n",
        "    h_previous_ts = torch.stack(h_next_ts)\n",
        "    logits = self.decode(h_previous_layer)\n",
        "    return logits, h_previous_ts\n",
        "\n",
        "  def generate(self, input, hidden, generated_seq_len, device):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        - input: A mini-batch of input tokens (NOT sequences!)\n",
        "                        shape: (batch_size)\n",
        "        - hidden: The initial hidden states for every layer of the stacked RNN.\n",
        "                        shape: (num_layers, batch_size, hidden_size)\n",
        "        - generated_seq_len: The length of the sequence to generate.\n",
        "                       Note that this can be different than the length used\n",
        "                       for training (self.seq_len)\n",
        "    Returns:\n",
        "        - Sampled sequences of tokens\n",
        "                    shape: (generated_seq_len, batch_size)\n",
        "    \"\"\"\n",
        "    # Model in eval mode\n",
        "    self.eval()\n",
        "\n",
        "    samples = []\n",
        "    h_previous_ts = hidden\n",
        "    new_input = input\n",
        "    for i in range(generated_seq_len):\n",
        "        new_input = new_input.to(device)\n",
        "        emb = self.embedding(new_input)\n",
        "        logits, h_previous_ts = self._forward_single_token_embedding(emb, h_previous_ts)\n",
        "        sample = self.softmax(logits)\n",
        "        sample_index = int(np.argmax(sample.cpu().detach().numpy()))\n",
        "        samples.append(sample_index)\n",
        "        new_input[0, 0] = sample_index\n",
        "    return samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ajtJmxFXlqy",
        "colab_type": "code",
        "outputId": "f89606bb-65a5-4aed-c67d-7f246b92dcff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "#!/bin/python\n",
        "# coding: utf-8\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from GRU_RNN import GRU\n",
        "#from simple_RNN import RNN\n",
        "\n",
        "\n",
        "# Use the GPU if you have one\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n",
        "      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#\n",
        "# DATA LOADING & PROCESSING\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def _read_words(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "      return f.read().replace(\"\\n\", \"<eos>\").split()\n",
        "\n",
        "def _build_vocab(filename):\n",
        "    data = _read_words(filename)\n",
        "\n",
        "    counter = collections.Counter(data)\n",
        "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "    words, _ = list(zip(*count_pairs))\n",
        "    word_to_id = dict(zip(words, range(len(words))))\n",
        "    id_to_word = dict((v, k) for k, v in word_to_id.items())\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "def _file_to_word_ids(filename, word_to_id):\n",
        "    data = _read_words(filename)\n",
        "    return [word_to_id[word] for word in data if word in word_to_id]\n",
        "\n",
        "# Processes the raw data from text files\n",
        "def ptb_raw_data(data_path=None, prefix=\"ptb\"):\n",
        "    train_path = os.path.join(data_path, prefix + \".train.txt\")\n",
        "    valid_path = os.path.join(data_path, prefix + \".valid.txt\")\n",
        "    test_path = os.path.join(data_path, prefix + \".test.txt\")\n",
        "\n",
        "    word_to_id, id_2_word = _build_vocab(train_path)\n",
        "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
        "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
        "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
        "    return train_data, valid_data, test_data, word_to_id, id_2_word\n",
        "\n",
        "# Yields minibatches of data\n",
        "def ptb_iterator(raw_data, batch_size, num_steps):\n",
        "    raw_data = np.array(raw_data, dtype=np.int32)\n",
        "\n",
        "    data_len = len(raw_data)\n",
        "    batch_len = data_len // batch_size\n",
        "    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
        "    for i in range(batch_size):\n",
        "        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
        "\n",
        "    epoch_size = (batch_len - 1) // num_steps\n",
        "\n",
        "    if epoch_size == 0:\n",
        "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
        "\n",
        "    for i in range(epoch_size):\n",
        "        x = data[:, i*num_steps:(i+1)*num_steps]\n",
        "        y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
        "        yield (x, y)\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    \"Data processing for the transformer. This class adds a mask to the data.\"\n",
        "    def __init__(self, x, pad=-1):\n",
        "        self.data = x\n",
        "        self.mask = self.make_mask(self.data, pad)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_mask(data, pad):\n",
        "        \"Create a mask to hide future words.\"\n",
        "\n",
        "        def subsequent_mask(size):\n",
        "            \"\"\" helper function for creating the masks. \"\"\"\n",
        "            attn_shape = (1, size, size)\n",
        "            subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "            return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "        mask = (data != pad).unsqueeze(-2)\n",
        "        mask = mask & Variable(\n",
        "            subsequent_mask(data.size(-1)).type_as(mask.data))\n",
        "        return mask\n",
        "\n",
        "\n",
        "# LOAD DATA\n",
        "print('Loading data from '+ 'data')\n",
        "raw_data = ptb_raw_data(data_path='data')\n",
        "train_data, valid_data, test_data, word_to_id, id_2_word = raw_data\n",
        "vocab_size = len(word_to_id)\n",
        "print('  vocabulary size: {}'.format(vocab_size))\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#\n",
        "# MODEL SETUP\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#\n",
        "# DEFINE COMPUTATIONS FOR PROCESSING ONE EPOCH\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"\n",
        "    Wraps hidden states in new Tensors, to detach them from their history.\n",
        "    This prevents Pytorch from trying to backpropagate into previous input\n",
        "    sequences when we use the final hidden states from one mini-batch as the\n",
        "    initial hidden states for the next mini-batch.\n",
        "    Using the final hidden states in this way makes sense when the elements of\n",
        "    the mini-batches are actually successive subsequences in a set of longer sequences.\n",
        "    This is the case with the way we've processed the Penn Treebank dataset.\n",
        "    \"\"\"\n",
        "    if isinstance(h, Variable):\n",
        "        return h.detach_()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "def compute_average_grad(model, data, model_name):\n",
        "    model.eval()\n",
        "\n",
        "    if model_name != 'TRANSFORMER':\n",
        "        hidden = model.init_hidden()\n",
        "        hidden = hidden.to(device)\n",
        "\n",
        "    for step, (x, y) in enumerate(ptb_iterator(data, model.batch_size, model.seq_len)):\n",
        "        # Prepare data\n",
        "        inputs = torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().to(device)\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        # Forward pass\n",
        "        outputs, hidden = model(inputs, hidden, keep_hiddens=True)\n",
        "        targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().to(device)\n",
        "        # Resize\n",
        "        tt = torch.squeeze(targets.view(-1, model.batch_size * model.seq_len))\n",
        "        out = outputs.contiguous().view(-1, model.vocab_size)\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(out, tt)\n",
        "        # Get the euclidian norm of the gradients with respect to each hidden layer\n",
        "        grads = [float(torch.autograd.grad(loss, hidden, retain_graph=True)[0].norm(2).cpu().numpy()) for hidden in model.hiddens]\n",
        "        # Return the result for the first and second hidden layer separately\n",
        "        \n",
        "        return grads[0::2], grads[1::2]\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#\n",
        "# RUN MAIN LOOP (TRAIN AND VAL)\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "print(\"\\n########## Running Main Loop ##########################\")\n",
        "\n",
        "\n",
        "# MAIN LOOP\n",
        "\n",
        "RNN_PATH = 'RNN_ADAM_model=RNN_optimizer=ADAM_initial_lr=0.0001_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.6_save_best_0'\n",
        "GRU_PATH = 'GRU_SGD_LR_SCHEDULE_model=GRU_optimizer=SGD_LR_SCHEDULE_initial_lr=10_batch_size=20_seq_len=35_hidden_size=1500_num_layers=2_dp_keep_prob=0.35_save_best_0'\n",
        "\n",
        "EMB_SIZE_RNN = 200\n",
        "HIDDEN_SIZE_RNN = 1500\n",
        "SEQ_LEN_RNN = 35\n",
        "BATCH_SIZE_RNN = 20\n",
        "VOCAB_SIZE_RNN = 10000\n",
        "NUM_LAYERS_RNN = 2\n",
        "DP_KEEP_PROB_RNN = 0.6\n",
        "\n",
        "EMB_SIZE_GRU = 200\n",
        "HIDDEN_SIZE_GRU = 1500\n",
        "SEQ_LEN_GRU = 35\n",
        "BATCH_SIZE_GRU = 20\n",
        "VOCAB_SIZE_GRU = 10000\n",
        "NUM_LAYERS_GRU = 2\n",
        "DP_KEEP_PROB_GRU = 0.35\n",
        "\n",
        "# Use the GPU if you have one\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "word_to_id, id_2_word = _build_vocab(os.path.join('data', 'ptb' + \".train.txt\"))\n",
        "\n",
        "# LOSS FUNCTION\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "model_classes = [RNN, GRU]\n",
        "for model_class in model_classes:\n",
        "    if model_class == RNN:\n",
        "        MODEL_PATH = RNN_PATH\n",
        "        model = model_class(emb_size=EMB_SIZE_RNN,\n",
        "                    hidden_size=HIDDEN_SIZE_RNN,\n",
        "                    seq_len=SEQ_LEN_RNN,\n",
        "                    batch_size=BATCH_SIZE_RNN,\n",
        "                    vocab_size=VOCAB_SIZE_RNN,\n",
        "                    num_layers=NUM_LAYERS_RNN,\n",
        "                    dp_keep_prob=DP_KEEP_PROB_RNN)\n",
        "        model_name = 'RNN'\n",
        "\n",
        "    if model_class == GRU:\n",
        "        MODEL_PATH = GRU_PATH\n",
        "        model = model_class(emb_size=EMB_SIZE_GRU,\n",
        "                    hidden_size=HIDDEN_SIZE_GRU,\n",
        "                    seq_len=SEQ_LEN_GRU,\n",
        "                    batch_size=BATCH_SIZE_GRU,\n",
        "                    vocab_size=VOCAB_SIZE_GRU,\n",
        "                    num_layers=NUM_LAYERS_GRU,\n",
        "                    dp_keep_prob=DP_KEEP_PROB_GRU)\n",
        "        model_name = 'GRU'\n",
        "\n",
        "    load_path = os.path.join(MODEL_PATH, 'best_params.pt')\n",
        "    model.load_state_dict(torch.load(load_path))\n",
        "\n",
        "    model = model.to(device)\n",
        "    first_layer_grads, second_layer_grads = compute_average_grad(model, train_data, model_name)\n",
        "\n",
        "    print(model_name)\n",
        "    print('First layer: {}'.format(first_layer_grads))\n",
        "    print('Second layer: {}'.format(second_layer_grads))\n",
        "    if model_name == 'RNN':\n",
        "        RNN_first_layer_grad = first_layer_grads\n",
        "        RNN_second_layer_grad = second_layer_grads\n",
        "    if model_name == 'GRU':\n",
        "        GRU_first_layer_grad = first_layer_grads\n",
        "        GRU_second_layer_grad = second_layer_grads\n",
        "        \n",
        "\n",
        "# plt.plot(loss_array, '-o', label=model_name)\n",
        "\n",
        "# plt.title(\"Norm of Gradients Vs. Timesteps\")\n",
        "# plt.ylabel(\"Norm of Gradients\")\n",
        "# plt.xlabel(\"Timestep\")\n",
        "# plt.legend()\n",
        "# plt.savefig(\"Q5.2_PLOT.jpg\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the GPU\n",
            "Loading data from data\n",
            "  vocabulary size: 10000\n",
            "\n",
            "########## Running Main Loop ##########################\n",
            "Using the GPU\n",
            "RNN\n",
            "First layer: [0.03999697417020798, 0.039250969886779785, 0.0377032607793808, 0.03668290004134178, 0.03544052317738533, 0.03535144403576851, 0.0340891033411026, 0.033654630184173584, 0.03432830423116684, 0.03415659815073013, 0.034264806658029556, 0.034579094499349594, 0.03409448638558388, 0.03160100430250168, 0.031142782419919968, 0.030931876972317696, 0.031229566782712936, 0.03189348056912422, 0.03214060515165329, 0.03141254186630249, 0.03156454488635063, 0.03164895996451378, 0.03168894350528717, 0.030662769451737404, 0.0304762851446867, 0.029834257438778877, 0.029254434630274773, 0.029751453548669815, 0.03095933049917221, 0.030185464769601822, 0.028158720582723618, 0.027110319584608078, 0.024655411019921303, 0.020492292940616608, 0.015138499438762665]\n",
            "Second layer: [0.018962986767292023, 0.01905764825642109, 0.018883123993873596, 0.01870482787489891, 0.018415672704577446, 0.018138350918889046, 0.017543654888868332, 0.016890697181224823, 0.01738726906478405, 0.018155096098780632, 0.017749102786183357, 0.017869794741272926, 0.018972523510456085, 0.01714433543384075, 0.016231151297688484, 0.016914047300815582, 0.01618473418056965, 0.017207486554980278, 0.01736305095255375, 0.01799427717924118, 0.01624809391796589, 0.016471758484840393, 0.01704009436070919, 0.017076943069696426, 0.016119690611958504, 0.014917543157935143, 0.014844732359051704, 0.014256728813052177, 0.015288003720343113, 0.015860099345445633, 0.014770451001822948, 0.015872322022914886, 0.0132985794916749, 0.012660620734095573, 0.010860655456781387]\n",
            "GRU\n",
            "First layer: [0.044003378599882126, 0.032249391078948975, 0.029060035943984985, 0.027540801092982292, 0.025749625638127327, 0.027421725913882256, 0.025474658235907555, 0.02506210468709469, 0.025165464729070663, 0.023937083780765533, 0.02535611391067505, 0.02312363311648369, 0.023711370304226875, 0.02275201492011547, 0.02194608561694622, 0.02139049768447876, 0.02310994639992714, 0.022192994132637978, 0.022834060713648796, 0.02449209988117218, 0.025280198082327843, 0.02247391827404499, 0.020432941615581512, 0.02150130085647106, 0.02037489227950573, 0.019079890102148056, 0.018342768773436546, 0.018242893740534782, 0.018310006707906723, 0.018867071717977524, 0.0176934115588665, 0.016010547056794167, 0.015309111215174198, 0.012370308861136436, 0.009482522495090961]\n",
            "Second layer: [0.06197084113955498, 0.0613132007420063, 0.05977693200111389, 0.05859242379665375, 0.05877221003174782, 0.05717400833964348, 0.05557139962911606, 0.05411083623766899, 0.05407175049185753, 0.05309329926967621, 0.05202358588576317, 0.051499929279088974, 0.05025492236018181, 0.04835989698767662, 0.0476086251437664, 0.04745819792151451, 0.04674346372485161, 0.046792615205049515, 0.04553591459989548, 0.04552188515663147, 0.04353158175945282, 0.041595760732889175, 0.04121784493327141, 0.03886621445417404, 0.03701154515147209, 0.03579018637537956, 0.03443652391433716, 0.03357376903295517, 0.03439422324299812, 0.032014571130275726, 0.0306452177464962, 0.02927953004837036, 0.02393644116818905, 0.020048314705491066, 0.013734041713178158]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CIxBUBXMYVHT",
        "colab_type": "code",
        "outputId": "b841bf94-7116-45d2-e024-763106862aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "RNN_norm = []\n",
        "GRU_norm = []\n",
        "\n",
        "for i in range(35):\n",
        "    rnn_norm = np.linalg.norm([RNN_first_layer_grad[i], RNN_second_layer_grad[i]])\n",
        "    RNN_norm.append(rnn_norm)\n",
        "    gru_norm = np.linalg.norm([GRU_first_layer_grad[i], GRU_second_layer_grad[i]])\n",
        "    GRU_norm.append(gru_norm)\n",
        "\n",
        "plt.plot(RNN_norm, '-o', label='RNN')\n",
        "plt.plot(GRU_norm, '-o', label='GRU')\n",
        "\n",
        "plt.title(\"Norm of Gradients Vs. Timesteps\")\n",
        "plt.ylabel(\"Norm of Gradients\")\n",
        "plt.xlabel(\"Timestep\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7nl2m_19ZKmK",
        "colab_type": "code",
        "outputId": "d8ef6d28-992c-4034-9009-c4b9969da662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;32mexp_config.txt\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HNy5C69udGSU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}